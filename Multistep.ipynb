{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75ca790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import utils\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc375d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df = pd.DataFrame(utils.get_config_dicts())\n",
    "base_models = sorted(set(config_df[\"base_model\"].values))\n",
    "datasets = sorted(set(config_df[\"dataset\"].values))\n",
    "drop_scoring_base_bools = [False, True]\n",
    "ensemble_configs = list(dict(zip((\"base_model\", \"drop_scoring_base\", \"dataset\"), p)) for p in itertools.product(base_models, drop_scoring_base_bools, datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "83c09865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step(df_train, df_test):\n",
    "    def cond_mean(scores, thresholds):\n",
    "        i = 0\n",
    "        while i < len(thresholds) and scores[i] < thresholds[i]:\n",
    "            i+=1\n",
    "        return np.mean(scores[:(i+1)])\n",
    "    \n",
    "    df_test[\"ms\"] = 0\n",
    "    auc_scores = []\n",
    "    for z in np.arange(0, 3, 0.1):\n",
    "        thresholds = [np.mean(df_train.loc[df_train['y'] == 1, model]) - z*np.std(df_train.loc[df_train['y'] == 1, model]) for model in df_train.columns.values[:-2]]\n",
    "        for i in range(df_train.shape[0]):\n",
    "            df_train.loc[i, \"ms\"] = cond_mean(df_train.iloc[i, :df_train.shape[1]-2].to_list(), thresholds)\n",
    "        auc_scores.append(roc_auc_score(df_train[\"y\"], df_train[\"ms\"]))\n",
    "    \n",
    "    z_star = 0.1*np.argmax(auc_scores)\n",
    "    thresholds = [np.mean(df_train.loc[df_train['y'] == 1, model]) - z_star*np.std(df_train.loc[df_train['y'] == 1, model]) for model in df_train.columns.values[:-2]]\n",
    "    for i in range(df_test.shape[0]):\n",
    "        df_test.loc[i, \"ms\"] = cond_mean(df_test.iloc[i, :df_test.shape[1]-2].to_list(), thresholds)\n",
    "    return df_test[\"ms\"]\n",
    "\n",
    "def get_aucs(**kwargs):\n",
    "    df = utils.get_pred_df(**kwargs)\n",
    "    \n",
    "    y = df[\"y\"]\n",
    "    X = df.drop(\"y\", axis=1).values\n",
    "\n",
    "    auc_dict = {}\n",
    "\n",
    "    auc_dict[\"max\"] = roc_auc_score(y, X.max(axis=1))\n",
    "    auc_dict[\"mean\"] =  roc_auc_score(y, X.mean(axis=1))\n",
    "    auc_dict[\"median\"] = roc_auc_score(y, np.median(X, axis=1))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    auc_dict[\"lr\"] = roc_auc_score(y_test, model_lr.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    model_rf = RandomForestClassifier()\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    auc_dict[\"rf\"] = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    df_train = pd.concat([pd.DataFrame(X_train, columns=df.columns.values[:-1]), pd.DataFrame(np.array(y_train), columns=[\"y\"])], axis=1)\n",
    "    df_test = pd.concat([pd.DataFrame(X_test, columns=df.columns.values[:-1]), pd.DataFrame(np.array(y_test), columns=[\"y\"])], axis=1)\n",
    "    auc_dict[\"ms\"] = roc_auc_score(y_test, multi_step(df_train, df_test))\n",
    "    \n",
    "    return auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15ca6130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max': 0.7833, 'mean': 0.80465, 'median': 0.732375, 'lr': 1.0, 'rf': 1.0, 'ms': 0.9895330112721417}\n",
      "                base_model  drop_scoring_base dataset      max      mean  \\\n",
      "0  EleutherAI-gpt-neo-125m              False    xsum  0.78330  0.804650   \n",
      "1  EleutherAI-gpt-neo-125m               True    xsum  0.31355  0.464975   \n",
      "2                     gpt2              False    xsum  0.81760  0.898925   \n",
      "3                     gpt2               True    xsum  0.55780  0.785875   \n",
      "4              gpt2-medium              False    xsum  0.92520  0.929075   \n",
      "5              gpt2-medium               True    xsum  0.89935  0.911500   \n",
      "\n",
      "     median        lr        rf        ms  \n",
      "0  0.732375  1.000000  1.000000  0.989533  \n",
      "1  0.464975  0.991971  0.975110  0.269771  \n",
      "2  0.890325  1.000000  1.000000  0.948379  \n",
      "3  0.785875  0.962424  0.948485  0.933737  \n",
      "4  0.899400  0.898638  0.893229  0.863381  \n",
      "5  0.911500  0.903266  0.896652  0.892104  \n",
      "drop_scoring_base    0.500000\n",
      "max                  0.716133\n",
      "mean                 0.799167\n",
      "median               0.780742\n",
      "lr                   0.959383\n",
      "rf                   0.952246\n",
      "ms                   0.816151\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcyskr/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3438: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_ensemble_results_nested = pd.DataFrame([{**c, \"aucs\":get_aucs(**c)} for c in ensemble_configs])\n",
    "df_ensemble_results = df_ensemble_results_nested.join(df_ensemble_results_nested[\"aucs\"].apply(pd.Series)).drop(\"aucs\", axis=1)\n",
    "print(df_ensemble_results_nested['aucs'][0])\n",
    "print(df_ensemble_results)\n",
    "print(np.mean(df_ensemble_results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9cff187f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4264/872822058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensemble_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"base_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scoring_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "print(ensemble_configs.sort_values([\"base_model\", \"scoring_model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6444c0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_model</th>\n",
       "      <th>scoring_model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.696050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.247875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.932550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.978875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.536425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>EleutherAI-gpt-neo-125m</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.899875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.901775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>xsum</td>\n",
       "      <td>0.925200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                base_model            scoring_model dataset       auc\n",
       "0  EleutherAI-gpt-neo-125m  EleutherAI-gpt-neo-125m    xsum  0.998200\n",
       "2  EleutherAI-gpt-neo-125m                     gpt2    xsum  0.696050\n",
       "1  EleutherAI-gpt-neo-125m              gpt2-medium    xsum  0.247875\n",
       "6                     gpt2  EleutherAI-gpt-neo-125m    xsum  0.932550\n",
       "8                     gpt2                     gpt2    xsum  0.978875\n",
       "7                     gpt2              gpt2-medium    xsum  0.536425\n",
       "3              gpt2-medium  EleutherAI-gpt-neo-125m    xsum  0.899875\n",
       "5              gpt2-medium                     gpt2    xsum  0.901775\n",
       "4              gpt2-medium              gpt2-medium    xsum  0.925200"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_auc_single().sort_values([\"base_model\", \"scoring_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95e43c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop_scoring_base    0.500000\n",
       "max                  0.670846\n",
       "mean                 0.771654\n",
       "median               0.758104\n",
       "lr                   0.951975\n",
       "rf                   0.961240\n",
       "ms                   0.815915\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_ensemble_results, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
